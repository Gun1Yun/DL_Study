{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn-uci.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPuSiQYBJ6Rkr+SX3AXgEX8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d7fHloF82kUy"},"source":["## For 'import ipynb files'\n","1. install import_ipynb, PyDrive\n","2. import ipynb files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TS1NKtd2H3G","executionInfo":{"status":"ok","timestamp":1621891103894,"user_tz":-540,"elapsed":32804,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}},"outputId":"22b12889-7c15-4ee2-ccef-8defc498402e"},"source":["!pip install -U -q PyDrive\n","!pip install import_ipynb\n","import import_ipynb\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# id for import files\n","'''\n","layers.ipynb : 1TE_v-Os9bcDepAWODmnDi5WuAz2tyfYn\n","optim.ipynb : 1kCLfroPeioTdHqogFoOkTVyFYsYe9RAh\n","'''\n","# test import\n","module = drive.CreateFile({'id':'1b1teolDFHMLhucgVFL_-2wjOyFoFzGmJ'})\n","module.GetContentFile('test_function.ipynb')\n","from test_function import print_test\n","print_test()\n","\n","# import require module\n","module = drive.CreateFile({'id':'1TE_v-Os9bcDepAWODmnDi5WuAz2tyfYn'})\n","module.GetContentFile('layers.ipynb')\n","module = drive.CreateFile({'id':'1kCLfroPeioTdHqogFoOkTVyFYsYe9RAh'})\n","module.GetContentFile('optim.ipynb')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting import_ipynb\n","  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=1286e5422a2f7538a484860ed822c0b96fb70b5d744e2dcaa324ba1541830708\n","  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n","importing Jupyter notebook from test_function.ipynb\n","test\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ee_4Bbc18xOA"},"source":["### GPU config\n","- True : cupy\n","- False : numpy"]},{"cell_type":"code","metadata":{"id":"zVkeyXGN8wxt","executionInfo":{"status":"ok","timestamp":1621891105806,"user_tz":-540,"elapsed":1927,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["GPU = True\n","\n","if GPU:\n","    import cupy as np\n","    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n","else:\n","    import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqmMBR7e9F65"},"source":["## import requirements"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21-9aNMN71db","executionInfo":{"status":"ok","timestamp":1621891114495,"user_tz":-540,"elapsed":8698,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}},"outputId":"7fbc8318-f43a-42eb-cada-3fb44fabf24e"},"source":["from layers import Convolution, Pooling, FullyConnected\n","from optim import Adam\n","\n","# for time series split\n","!pip install scikit-learn==0.24.2"],"execution_count":3,"outputs":[{"output_type":"stream","text":["importing Jupyter notebook from layers.ipynb\n","importing Jupyter notebook from optim.ipynb\n","Collecting scikit-learn==0.24.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.0.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.19.5)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wn3eJ3Mq8O0v","executionInfo":{"status":"ok","timestamp":1621891869277,"user_tz":-540,"elapsed":560,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["# MSE for Loss\n","def MSE(y, t):\n","    return 0.5 * np.mean((y-t)**2)\n","\n","# ReLU activation function\n","class ReLU:\n","    def __init__(self):\n","        self.mask = None\n","    \n","    def forward(self, x):\n","        self.mask = (x<=0)\n","        y = x.copy()\n","        y[self.mask] = 0\n","        return y\n","\n","    def backward(self, dy):\n","        dy[self.mask] = 0\n","        dx = dy\n","        return dx\n","\n","# Loss with ReLU\n","class ReluWithLoss:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.activation = ReLU()\n","        self.cache = None\n","\n","    def forward(self, x, t):\n","        N, V = x.shape      # batch, output\n","        \n","        x = x.reshape(N, V)\n","        t = t.reshape(N, V)\n","        x = self.activation.forward(x)\n","\n","        loss = MSE(x, t)\n","        self.cache = (t, x, (N, V))\n","        return loss\n","\n","    def backward(self, dy=1):\n","        t, x, (N, V) = self.cache\n","        dx = dy * (x-t) / N\n","        \n","        dx = self.activation.backward(dx)\n","        dx = dx.reshape(N, V)\n","        return dx"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"3XEMBqOf_0rb","executionInfo":{"status":"ok","timestamp":1621891912279,"user_tz":-540,"elapsed":388,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["# CNN Model with Regression\n","class CnnModelReg:\n","    def __init__(self, input_dim = (1, 24, 8), \n","                 params={'filter_num':30, 'filter_size':5, \n","                         'padding':0, 'stride':1},\n","                 hidden_size=100, output_size=1):\n","        filter_num = params['filter_num']\n","        filter_size = params['filter_size']\n","        padding = params['padding']\n","        stride = params['stride']\n","\n","        # not square\n","        input_size = input_dim[1]\n","        input_size2 = input_dim[2]\n","        conv_output_size_h = (input_size - filter_size + 2*padding)/stride + 1\n","        conv_output_size_w = (input_size2 - filter_size + 2*padding)/stride + 1\n","        pool_output_size = int(filter_num*(conv_output_size_h/2)*(conv_output_size_w/2))\n","\n","        self.params = {}\n","        rand = np.random.randn\n","\n","        # He initialize\n","        self.params['cnn_W'] = rand(filter_num, input_dim[0], filter_size, filter_size) / np.sqrt(filter_num/2)\n","        self.params['cnn_b'] = np.zeros(filter_num)\n","        self.params['W1'] = rand(pool_output_size, hidden_size)/np.sqrt(pool_output_size/2)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = rand(hidden_size, output_size)/np.sqrt(hidden_size/2)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","        self.layers=[\n","                     Convolution(self.params['cnn_W'], self.params['cnn_b'], stride, padding),\n","                     ReLU(),\n","                     Pooling(pool_h=2, pool_w=2, stride=2),\n","                     FullyConnected(self.params['W1'], self.params['b1']),\n","                     ReLU(),\n","                     FullyConnected(self.params['W2'], self.params['b2']),\n","        ]\n","        self.loss_layer = ReluWithLoss()\n","\n","        self.grads = []\n","        self.params = []\n","        for i in [0, 3, 5]:\n","            self.params += self.layers[i].params\n","            self.grads+=self.layers[i].grads\n","\n","    def predict(self, x):\n","        x = np.array(x)\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def forward(self, x, t):\n","        x = np.array(x)\n","        t = np.array(t)\n","        x = self.predict(x)\n","        loss = self.loss_layer.forward(x, t)\n","        return loss\n","\n","    def backward(self, dy=1):\n","        dy = self.loss_layer.backward(dy)\n","\n","        for layer in reversed(self.layers):\n","            dy = layer.backward(dy)\n","        return dy\n","    \n","    def fit(self, train_X=None, train_y=None,learning_rate=0.01, epochs=10, batch_size=32, verbose=0):\n","        optimizer = Adam(learning_rate)\n","\n","        data_size = train_X.shape[0]\n","        max_iters = data_size//batch_size\n","\n","        for epoch in range(1, epochs+1):\n","            # shuffle train data\n","            idx = numpy.random.permutation(numpy.arange(data_size))\n","            x_data = train_X[idx]\n","            y_data = train_y[idx]\n","\n","            epoch_loss = 0\n","            start_time = time.time()\n","            for iter in range(max_iters):\n","                batch_x = x_data[iter*batch_size:(iter+1)*batch_size]\n","                batch_y = y_data[iter*batch_size:(iter+1)*batch_size]\n","\n","                loss = self.forward(batch_x, batch_y)\n","                self.backward()\n","                params, grads = self.params, self.grads\n","                optimizer.update(params, grads)\n","\n","                epoch_loss += loss\n","            avg_loss = epoch_loss/max_iters\n","\n","            if verbose:\n","                duration = start_time-time.time()\n","                print(f'epoch:{epoch}/{epochs}, 시간:{duration:.2f}[s], loss:{avg_loss:.5f}')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"lx--r5IgWSPR","executionInfo":{"status":"ok","timestamp":1621892488456,"user_tz":-540,"elapsed":295,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["# series data to img function\n","def series_to_img(dataset, time_step=1):\n","    num = dataset.shape[1]      # features num\n","    df = pd.DataFrame(dataset)\n","    cols, names = list(), list()\n","    # sequence t-n to t-1\n","    for i in range(time_step, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [('var%d(t-%d)' % (j+1, i)) for j in range(num)]\n","\n","    for i in range(0, 1):\n","        cols.append(df.shift(-i))\n","        if i == 0:\n","            names += [('var%d(t)' % (j+1)) for j in range(num)]\n","        else:\n","            names += [('var%d(t+%d)' % (j+1, i)) for j in range(num)]\n","\n","    agg = pd.concat(cols, axis=1)\n","    agg.columns = names\n","    agg.dropna(inplace=True)\n","    return agg\n","\n","# configuration setting\n","def model_config():\n","    # parameter for CNN Model\n","    filter_num = [30]\n","    filter_size = [3, 5]\n","    epochs = [10]\n","    batch_size = [64]\n","    learning_rate = [0.1, 0.01]\n","    \n","    # create config data\n","    configs = []\n","    for i in filter_num:\n","        for j in filter_size:\n","            for k in epochs:\n","                for l in batch_size:\n","                    for m in learning_rate:\n","                        config = [i, j, k, l, m]\n","                        configs.append(config)\n","    return configs\n","\n","# fucntion for fit cnn model using configs\n","def model_fit(train_X, train_y, config):\n","    # unpack config\n","    n_filter, n_fsize, n_epochs, n_batch, learning_rate = config\n","    cnn_params = {'filter_num':n_filter, 'filter_size':n_fsize, \n","                  'stride':1, 'padding':0}\n","    model = CnnModelReg(params=cnn_params)\n","    # fit model and return\n","    model.fit(train_X=train_X, train_y=train_y, epochs=n_epochs, \n","              batch_size=n_batch, learning_rate=learning_rate)\n","    return model\n","\n","def MAE_metric(x, t):\n","    return np.mean(numpy.abs(x-t))\n","\n","def MSE_metric(x, t):\n","    t = np.array(t)\n","    return np.mean((x-t)**2)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIYfIWbOYSi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621892831045,"user_tz":-540,"elapsed":221742,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}},"outputId":"38f0dfe6-232a-4b3b-edf6-0bbaf221afab"},"source":["# dataset\n","import pandas as pd\n","import numpy\n","import time\n","from datetime import datetime\n","\n","df_parser = lambda x: datetime.strptime(x, '%Y %m %d %H')    # string to datetime\n","# data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv'\n","data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv'\n","df = pd.read_csv(data_url, sep=',', parse_dates=[['year', 'month', 'day', 'hour']], date_parser=df_parser, index_col=0)\n","\n","del df['No']\n","df.columns = ['pm2.5', 'dewp', 'temp', 'pres', 'cbwd','wind_speed', 'snow', 'rain']\n","df = df[24:]            # NaN values in first 24hours\n","\n","# sklearn library for time series split\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","\n","dataset = df.values\n","label_encoder = LabelEncoder()\n","dataset[:, 4] = label_encoder.fit_transform(dataset[:, 4])  # for wind direction\n","dataset = dataset.astype('float')\n","\n","n_inputs = 24       # input time dim\n","n_features = 8      # input feature dim\n","del_idx = n_inputs * n_features + 1\n","del_cols = [i for i in range(del_idx, del_idx+n_features-1)]\n","new_df = series_to_img(dataset, n_inputs)       # time series to img data\n","new_df.drop(new_df.columns[del_cols], axis=1, inplace=True)\n","\n","n_splits = 3\n","train_test_split = TimeSeriesSplit(n_splits=n_splits+1, gap=n_inputs).split(new_df)\n","next(train_test_split)\n","\n","configs = model_config()\n","history = []\n","best_error = []\n","i = 1\n","\n","print('config : filter_num, filter_size, epochs, batch_size, learning_rate')\n","\n","# nested cross validation for time series model\n","for train_cv_indices, test_cv_indices in train_test_split:\n","    print(f'fold : {i}/{n_splits}')\n","    i+=1\n","\n","    # split x, y data\n","    train_cv_X, train_cv_y = new_df.iloc[train_cv_indices, :-1].values, new_df.iloc[train_cv_indices,-1].values\n","    test_cv_X, test_cv_y = new_df.iloc[test_cv_indices, :-1].values, new_df.iloc[test_cv_indices, -1].values\n","\n","    # length for validation set\n","    test_length = len(test_cv_X)\n","\n","    # scaling data\n","    scaler_x = MinMaxScaler()\n","    train_cv_X = scaler_x.fit_transform(train_cv_X)\n","    test_cv_X = scaler_x.transform(test_cv_X)\n","\n","    train_X, val_X = train_cv_X[:-test_length, :], train_cv_X[-test_length:, :]\n","    train_y, val_y = train_cv_y[:-test_length], train_cv_y[-test_length:]\n","\n","    # reshape\n","    # inner loop\n","    train_X = train_X.reshape(-1, 1, n_inputs, n_features)\n","    val_X = val_X.reshape(-1, 1, n_inputs, n_features)\n","    train_y = train_y.reshape(-1, 1)\n","    val_y = val_y.reshape(-1, 1)\n","\n","    # outer loop\n","    train_cv_X = train_cv_X.reshape(-1, 1, n_inputs, n_features)\n","    test_cv_X = test_cv_X.reshape(-1, 1, n_inputs, n_features)\n","    train_cv_y = train_cv_y.reshape(-1, 1)\n","    test_cv_y = test_cv_y.reshape(-1, 1)\n","\n","    # model fit, inner\n","    errors = []\n","    for idx, cfg in enumerate(configs):\n","        print(f' == train {cfg} model == ', end=' ')\n","        model = model_fit(train_X, train_y, cfg)\n","        predicted = model.predict(val_X)\n","        error = np.sqrt(MSE_metric(predicted, val_y))   # rmse\n","        print(f'error(rmse):{error.item():.2f}')\n","        if errors:\n","            if error < min(errors):\n","                param = idx\n","        else:\n","            param = idx\n","        errors.append(error)\n","\n","    history.append(errors)\n","\n","    # outer\n","    selected_model = model_fit(train_cv_X,train_cv_y, configs[param])\n","    predicted = selected_model.predict(test_cv_X)\n","    error = np.sqrt(MSE_metric(predicted, test_cv_y))\n","    best_error.append(error)\n","\n","    # model eval\n","    print(f'best_model => error(rmse) : {error.item():.2f}, param:{configs[param]}')"],"execution_count":30,"outputs":[{"output_type":"stream","text":["config : filter_num, filter_size, epochs, batch_size, learning_rate\n","fold : 1/3\n"," == train [30, 3, 10, 64, 0.1] model ==  error(rmse):37.99\n"," == train [30, 3, 10, 64, 0.01] model ==  error(rmse):35.66\n"," == train [30, 5, 10, 64, 0.1] model ==  error(rmse):89.52\n"," == train [30, 5, 10, 64, 0.01] model ==  error(rmse):142.59\n","best_model => error(rmse) : 26.41, param:[30, 3, 10, 64, 0.01]\n","fold : 2/3\n"," == train [30, 3, 10, 64, 0.1] model ==  error(rmse):128.21\n"," == train [30, 3, 10, 64, 0.01] model ==  error(rmse):26.70\n"," == train [30, 5, 10, 64, 0.1] model ==  error(rmse):85.51\n"," == train [30, 5, 10, 64, 0.01] model ==  error(rmse):27.15\n","best_model => error(rmse) : 28.64, param:[30, 3, 10, 64, 0.01]\n","fold : 3/3\n"," == train [30, 3, 10, 64, 0.1] model ==  error(rmse):40.34\n"," == train [30, 3, 10, 64, 0.01] model ==  error(rmse):30.16\n"," == train [30, 5, 10, 64, 0.1] model ==  error(rmse):82.20\n"," == train [30, 5, 10, 64, 0.01] model ==  error(rmse):129.89\n","best_model => error(rmse) : 22.61, param:[30, 3, 10, 64, 0.01]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fpEn96wPDDQA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621894760728,"user_tz":-540,"elapsed":282,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}},"outputId":"895c127b-0506-4a8c-af48-9a4028b75a6e"},"source":["model_evaluation = sum(best_error)\n","model_evaluation /= n_splits\n","print(f'evaluation [Mean RMSE] : {model_evaluation}')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["evaluation [Mean RMSE] : 25.888455177055764\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"biNStI8_ddBu","executionInfo":{"status":"ok","timestamp":1621894848759,"user_tz":-540,"elapsed":631,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}},"outputId":"224c79fb-9ac3-4763-a985-e2c16cea768f"},"source":["predicted = selected_model.predict(test_cv_X)\n","print(f'MSE : {MSE_metric(predicted, test_cv_y)}')\n","print(f'RMSE : {np.sqrt(MSE_metric(predicted, test_cv_y))}')\n","\n","def MAE_metric(x, t):\n","    t = np.array(t)\n","    return np.mean(numpy.abs(x-t))\n","print(f'MAE : {MAE_metric(predicted, test_cv_y)}')"],"execution_count":38,"outputs":[{"output_type":"stream","text":["MSE : 511.4273951674669\n","RMSE : 22.614760559587335\n","MAE : 13.7427807869428\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBiqB6pmdqTT"},"source":[""],"execution_count":null,"outputs":[]}]}