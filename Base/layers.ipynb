{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"layers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONi7e7bV8pzz5dUAtNmdsM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"OrP2FzIk_Vl3","executionInfo":{"status":"ok","timestamp":1621668771636,"user_tz":-540,"elapsed":520,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["GPU = False\n","\n","if GPU:\n","    import cupy as np\n","    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n","else:\n","    import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8TL1TFO0Jg9"},"source":["## Fully Connected Layer"]},{"cell_type":"code","metadata":{"id":"evhUEYTw0HId","executionInfo":{"status":"ok","timestamp":1621668772241,"user_tz":-540,"elapsed":9,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["class FullyConnected:\n","    def __init__(self, W, b):\n","        self.params = [W, b]\n","        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n","        self.x = None\n","\n","    def forward(self, x):\n","        W, b = self.params\n","        self.x_shape = x.shape\n","\n","        x = x.reshape(x.shape[0], -1)\n","        y = np.dot(x, W) + b        # y = X*W + b\n","        self.x = x\n","\n","        return y\n","\n","    def backward(self, dy):\n","        W, b = self.params\n","        x = self.x\n","\n","        db = np.sum(dy, axis=0)\n","        dW = np.dot(x.T, dy)\n","        dx = np.dot(dy, W.T)\n","\n","        self.grads[0][...] = dW\n","        self.grads[1][...] = db\n","        dx = dx.reshape(*self.x_shape)\n","\n","        return dx"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drIkL5wn0lEV"},"source":["## CNN Layer\n","- im2col, col2im function\n","- Conv, Pool layer"]},{"cell_type":"code","metadata":{"id":"4maoVyOz0iae","executionInfo":{"status":"ok","timestamp":1621668772242,"user_tz":-540,"elapsed":8,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["def im2col(data, filter_h, filter_w, stride = 1, padding = 0):\n","    # flatten data to 2D array\n","    N, C, H, W = data.shape\n","    \n","    out_h = (H + 2*padding - filter_h)//stride + 1\n","    out_w = (W + 2*padding - filter_w)//stride + 1\n","\n","    # padding for H, W\n","    img = np.pad(data, \n","                 [(0, 0), (0, 0), (padding, padding), (padding, padding)], \n","                 'constant')\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","            \n","    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n","\n","    return col\n","\n","def col2im(col, shape, filter_h, filter_w, stride=1, padding=0):\n","    # 2D for img data\n","    # shape = original data shape\n","    N, C, H, W = shape\n","    out_h = (H + 2*padding - filter_h)//stride + 1\n","    out_w = (W + 2*padding - filter_w)//stride + 1\n","    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n","\n","    img = np.zeros((N, C, H+2*padding + stride -1, W+2*padding+stride-1))\n","    \n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            img[:, :, y:y_max:stride, x:x_max:stride] =+ col[:, :, y, x, :, :]\n","\n","    return img[:, :, padding:H+padding, padding:W+padding]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXlpl3Xe01vB","executionInfo":{"status":"ok","timestamp":1621668772243,"user_tz":-540,"elapsed":7,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["class Convolution:\n","    def __init__(self, W, b, stride=1, padding=0):\n","        self.params = [W, b]\n","        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n","        self.stride = stride\n","        self.padding = padding\n","        self.cache = None\n","\n","    def forward(self, x):\n","        weight, b = self.params\n","        FN, FC, FH, FW = weight.shape\n","        N, C, H, W = x.shape\n","\n","        out_h = (H + 2*self.padding - FH)//self.stride + 1\n","        out_w = (W + 2*self.padding - FW)//self.stride + 1\n","\n","        col = im2col(x, FH, FW, self.stride, self.padding)\n","        col_W = weight.reshape(FN, -1).T\n","\n","        y = np.dot(col, col_W) + b\n","        y = y.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n","\n","        self.cache = (x, col, col_W)\n","\n","        return y\n","\n","    def backward(self, dy):\n","        W, b = self.params\n","        x, col, col_W = self.cache\n","        FN, C, FH, FW = W.shape\n","        \n","        dy = dy.transpose(0, 2, 3, 1).reshape(-1, FN)\n","\n","        db = np.sum(dy, axis=0)\n","        dW = np.dot(col.T, dy)\n","        dW = dW.transpose(1, 0).reshape(FN, C, FH, FW)\n","\n","        self.grads[0][...] = dW\n","        self.grads[1][...] = db\n","\n","        dx = np.dot(dy, col_W.T)\n","        dx = col2im(dx, x.shape, FH, FW, self.stride, self.padding)\n","\n","        return dx\n","\n","class Pooling:\n","    def __init__(self, pool_h, pool_w, stride=1, padding=0):\n","        self.pool_h = pool_h\n","        self.pool_w = pool_w\n","        self.stride = stride\n","        self.padding = padding\n","\n","    def forward(self, x):\n","        N, C, H, W = x.shape\n","        out_h = int(1 + (H - self.pool_h) / self.stride)\n","        out_w = int(1 + (W - self.pool_w) / self.stride)\n","\n","        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.padding)\n","        col = col.reshape(-1, self.pool_h*self.pool_w)\n","\n","        arg_max = np.argmax(col, axis=1)\n","        y = np.max(col, axis=1)\n","        y = y.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n","\n","        self.cache = (x, arg_max)\n","\n","        return y\n","\n","    def backward(self, dy):\n","        x, arg_max = self.cache\n","\n","        dy = dy.transpose(0, 2, 3, 1)      # N, C, H, W\n","        pool_size = self.pool_h*self.pool_w\n","\n","        dmax = np.zeros((dy.size, pool_size))\n","        dmax[np.arange(arg_max.size), arg_max.flatten()] = dy.flatten()\n","        dmax = dmax.reshape(dy.shape + (pool_size, ))\n","        dcol = dmax.reshape(dmax.shape[0]*dmax.shape[1]*dmax.shape[2], -1)\n","        dx = col2im(dcol, x.shape, \n","                    self.pool_h, \n","                    self.pool_w, \n","                    self.stride, \n","                    self.padding)\n","        \n","        return dx"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K81OFTxX1Rgf"},"source":["## LSTM layer"]},{"cell_type":"code","metadata":{"id":"BW1jE190078J","executionInfo":{"status":"ok","timestamp":1621668772573,"user_tz":-540,"elapsed":336,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":["class LSTM:\n","    def __init__(self, Wx, Wh, b):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.cache = None\n","\n","    def forward(self, x, h_prev, c_prev):\n","        # Affine transformation (Wx[f, g, i, o], Wh[f, g, i, o], b[f, g, i, o])\n","        Wx, Wh, b = self.params\n","        N, H = h_prev.shape\n","\n","        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b\n","\n","        # slice for gates and get\n","        forget = A[:, :H]       # NxH\n","        get = A[:, H:2*H]\n","        input = A[:, 2*H:3*H]\n","        output = A[:, 3*H:4*H]\n","\n","        forget = sigmoid(forget)   # forget gate\n","        get = np.tanh(get)        # new memory\n","        input = sigmoid(input)    # input gate\n","        output = sigmoid(output)    # output gate\n","\n","        c_next = (c_prev * forget) + (get * input)\n","        h_next = np.tanh(c_next) * output\n","\n","        self.cache = (x, h_prev, c_prev, input, forget, get, output, c_next)\n","        return h_next, c_next\n","\n","    def backward(self, dh_next, dc_next):\n","        Wx, Wh, b = self.params\n","        x, h_prev, c_prev, input, forget, get, output, c_next = self.cache\n","\n","        # chain rule\n","        do = dh_next * np.tanh(c_next)\n","        do_s = do * output*(1-output)\n","        dt = dh_next * output\n","        dt_c = dt * (1-(np.tanh(c_next)**2))\n","\n","        di = dt_c * get\n","        dg = dt_c * input\n","        di_s = di * input*(1-input)\n","        dg_t = dg * (1-(get**2))\n","\n","        dc_prev = dt_c * forget\n","        df = dt_c * c_prev\n","        df_s = df * forget*(1-forget)\n","\n","        dA = np.hstack((df_s, dg_t, di_s, do_s))\n","\n","        db = np.sum(dA, axis = 0)\n","        dWh = np.matmul(h_prev.T, dA)\n","        dh_prev = np.matmul(dA, Wh.T)\n","        dWx = np.matmul(x.T, dA)\n","        dx = np.matmul(dA, Wx.T)\n","\n","        self.grads[0][...] = dWx\n","        self.grads[1][...] = dWh\n","        self.grads[2][...] = db\n","\n","        return dx, dh_prev, dc_prev\n","\n","class TimeLSTM:\n","    def __init__(self, Wx, Wh, b, stateful=False):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.layers = None      # for LSTM layer\n","        self.h, self.c = None, None\n","        self.dh = None\n","        self.stateful = stateful\n","\n","    def set_state(self, h, c=None):\n","        self.h, self.c = h, c\n","\n","    def reset_state(self):\n","        self.h, self.c = None, None\n","\n","    def forward(self, xs):\n","        Wx, Wh, b = self.params\n","        N, T, D = xs.shape      # mini-batch, time length, Dimension\n","        self.T = T\n","        H = Wh.shape[0]         # Wh (H, 4H) H: hidden size\n","        \n","        self.layers = []        # for stacking LSTM layer (horizontal)\n","        hs = np.empty((N, T, H), dtype='f')   # for save (h0 ... ht)\n","        \n","        # if not stateful, initialize h and c\n","        if not self.stateful or self.h is None:\n","            self.h = np.zeros((N, H), dtype='f')\n","        if not self.stateful or self.c is None:\n","            self.c = np.zeros((N, H), dtype='f')\n","\n","        for t in range(T):\n","            layer = LSTM(*self.params)\n","            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n","            \n","            hs[:, t, :] = self.h\n","            self.layers.append(layer)\n","        # many to one\n","        hs = hs[:, -1, :] # return last states\n","        return hs\n","        \n","    def backward(self, dhs):\n","        Wx, Wh, b = self.params\n","        N, H = dhs.shape\n","        T = self.T\n","        D = Wx.shape[0]\n","\n","        dxs = np.empty((N, T, D), dtype='f')\n","        dh, dc = 0, 0\n","        dh = dhs\n","\n","        grads = [0, 0, 0]   #dWx, dWh, db\n","        for t in reversed(range(self.T)):  # BPTT\n","            layer = self.layers[t]\n","            # dx, dh, dc = layer.backward(dhs[:,t ,:] + dh, dc)\n","            dx, dh, dc = layer.backward(dh, dc)\n","            dxs[:, t, :] = dx\n","            for i, grad in enumerate(layer.grads):\n","                grads[i] += grad\n","\n","        for i, grad in enumerate(grads):\n","            self.grads[i][...] = grad\n","        \n","        self.dh = dh\n","        return dxs"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BWd5tLv1txS","executionInfo":{"status":"ok","timestamp":1621668772574,"user_tz":-540,"elapsed":15,"user":{"displayName":"윤건일","photoUrl":"","userId":"05157393038207945770"}}},"source":[""],"execution_count":5,"outputs":[]}]}