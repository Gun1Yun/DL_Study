{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GRU.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMAPHFAnrULUkm5EPkbdgeg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"VaR0YFUdvQhl"},"source":["import numpy as np\n","\n","\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","\n","class GRU(object):\n","    def __init__(self, Wx, Wh, b):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.cache = None\n","\n","    def affine(self, x, Wx, h, Wh, b):\n","        return np.matmul(x, Wx) + np.matmul(h, Wh) + b\n","\n","    def forward(self, x, h_prev):\n","        # GRU doesn't have cell state\n","        Wx_, Wh_, b_ = self.params\n","        N, H = h_prev.shape\n","\n","        Wxz, Wxr, Wx = Wx_[:, :H], Wx_[:, H:2*H], Wx_[:, 2*H:]\n","        Whz, Whr, Wh = Wh_[:, :H], Wh_[:, H:2*H], Wh_[:, 2*H:]\n","        bz, br, b = b_[:, :H], b_[:, H:2*H], b_[:, 2*H:]\n","\n","        z = sigmoid(self.affine(x, Wxz, h_prev, Whz, bz))\n","        r = sigmoid(self.affine(x, Wxr, h_prev, Whr, br))\n","        h_hat = np.tanh(self.affine(x, Wx, r*h_prev, Wh, b))\n","        h_next = (1-z)*h_prev, z*h_hat\n","\n","        self.cache = (x, h_prev, z, r, h_hat)\n","\n","        return h_next\n","\n","    def backward(self, dh_next):\n","        x, h_prev, z, r, h_hat = self.cache\n","        Wx_, Wh_, b_ = self.params\n","        N, H = h_prev.shape\n","\n","        Wxz, Wxr, Wx = Wx_[:, :H], Wx_[:, H:2*H], Wx_[:, 2*H:]\n","        Whz, Whr, Wh = Wh_[:, :H], Wh_[:, H:2*H], Wh_[:, 2*H:]\n","\n","        dh_hat = z*dh_next\n","        dh_prev = (1-z)*dh_next\n","\n","        # h_hat\n","        # y = h_hat\n","        # tanh diff = (1-y**2)\n","        dtanh = dh_hat*(1-h_hat**2)\n","        dx_hat = np.matmul(dtanh, Wx.T)\n","        dWx_hat = np.matmul(x.T, dtanh)\n","        dWh_hat = np.matmul((r*h_prev).T, dtanh)\n","        db_hat = np.sum(dtanh, axis=0)\n","\n","        dh_r = np.matmul(dtanh, Wh.T)\n","        dh_prev_hat = r*dh_r\n","\n","        # r\n","        dr = dh_r*h_prev\n","        drs = dr*r(1-r)\n","        dx_r = np.matmul(drs, Wxr.T)\n","        dWx_r = np.matmul(x.T, drs)\n","        dh_r = np.matmul(drs, Whr.T)\n","        dWh_r = np.matmul(h_prev.T, drs)\n","        db_r = np.sum(drs, axis=0)\n","\n","        # z\n","        dz = dh_next*h_hat - dh_next*h_prev\n","        dzs = dz*z(1-z)\n","        dx_z = np.matmul(dzs, Wxz.T)\n","        dWx_z = np.matmul(x.T, dzs)\n","        dh_z = np.matmul(dzs, Whz.T)\n","        dWh_z = np.matmul(h_prev.T, dzs)\n","        db_z = np.sum(dzs, axis=0)\n","\n","        dWx = np.hstack(dWx_hat, dWx_r, dWx_z)\n","        dWh = np.hstack(dWh_hat, dWh_r, dWh_z)\n","        db = np.hstack(db_hat, db_r, db_z)\n","\n","        dx = dx_hat + dx_r + dx_z\n","        dh_prev += (dh_hat + dh_r + dh_z)\n","\n","        self.grads[0][...] = dWx\n","        self.grads[1][...] = dWh\n","        self.grads[2][...] = db\n","\n","        return dx, dh_prev\n","\n","\n","class TimeGRU(object):\n","    def __init__(self, Wx, Wh, b, stateful=False):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.layers = None\n","        self.h = None\n","        self.dh = None\n","        self.stateful = stateful\n","\n","    def set_state(self, h):\n","        self.h = h\n","\n","    def reset_state(self):\n","        self.h = None\n","\n","    def forward(self, xs):\n","        # xs is x sequences\n","        Wx, Wh, b = self.params\n","        N, T, D = xs.shape  # batch, time step, dimension\n","        H = Wh.shape[0]     # hidden size\n","\n","        self.layers = []\n","        hs = np.empty((N, T, D), dtype='f')     # save h0 to ht\n","\n","        # initialize h\n","        if not self.stateful or self.h is None:\n","            self.h = np.zeros((N, H), dtype='f')\n","\n","        # loop for time step\n","        for t in range(T):\n","            layer = GRU(*self.params)\n","            self.h = layer.forward(xs[:, t, :], self.h)\n","            hs[:, t, :] = self.h\n","            self.layers.append(layer)\n","\n","        return hs\n","\n","    def backward(self, dhs=1):\n","        Wx, Wh, b = self.params\n","        N, T, H = dhs.shape\n","        D = Wx.shape[0]\n","\n","        dxs = np.empty((N, T, D), dtype='f')\n","        dh = 0\n","\n","        grads = [0, 0, 0]       # grads for Wx, Wh, b\n","        for t in reversed(range(T)):    # backpropagation through time\n","            layer = self.layers[t]\n","            dx, dh = layer.backward(dhs[:, t, :]+dh)\n","            dxs[:, t, :] = dx\n","            for i, grad in enumerate(layer.grads):\n","                grads[i] += grad\n","\n","        for i, grad in enumerate(grads):\n","            self.grads[i][...] = grad\n","\n","        self.dh = dh\n","        return dxs"],"execution_count":null,"outputs":[]}]}